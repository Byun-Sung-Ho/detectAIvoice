{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0c955-e7f6-4464-a400-08d893cddbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pythonProject2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7c500-eb37-4970-ab26-2e42b0280216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    # Dataset\n",
    "    ROOT_FOLDER = './'\n",
    "    # Training\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 20\n",
    "    LR = 3e-4\n",
    "    # Others\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Seed 고정\n",
    "df = pd.read_csv('train.csv')\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "def add_noise(y, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(y))\n",
    "    augmented_data = y + noise_factor * noise\n",
    "    return augmented_data\n",
    "\n",
    "def pitch_shift(y, sr, n_steps):\n",
    "    return librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n",
    "\n",
    "def time_stretch(y, rate):\n",
    "    return librosa.effects.time_stretch(y=y, rate=rate)\n",
    "\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "\n",
    "        if train_mode:\n",
    "            # Data augmentation\n",
    "            if random.random() < 0.5:\n",
    "                y = add_noise(y)\n",
    "            if random.random() < 0.5:\n",
    "                y = pitch_shift(y, sr, random.uniform(-2, 2))\n",
    "            if random.random() < 0.5:\n",
    "                y = time_stretch(y, random.uniform(0.8, 1.2))\n",
    "\n",
    "        # Feature extraction\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        spec_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        chroma = np.mean(chroma.T, axis=0)\n",
    "        spec_contrast = np.mean(spec_contrast.T, axis=0)\n",
    "        tonnetz = np.mean(tonnetz.T, axis=0)\n",
    "        rms = np.mean(rms.T, axis=0)\n",
    "\n",
    "        feature_vector = np.concatenate((mfcc, chroma, spec_contrast, tonnetz, rms))\n",
    "        features.append(feature_vector)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features\n",
    "\n",
    "train_mfcc, train_labels = get_mfcc_feature(train, True)\n",
    "val_mfcc, val_labels = get_mfcc_feature(val, True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label, sequence_length=1):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mfcc = self.mfcc[index]\n",
    "        mfcc = mfcc.reshape(self.sequence_length, -1)  # Reshape to (sequence_length, input_dim)\n",
    "        if self.label is not None:\n",
    "            label = self.label[index]\n",
    "            return mfcc, label\n",
    "        return mfcc\n",
    "\n",
    "train_dataset = CustomDataset(train_mfcc, train_labels, sequence_length=1)\n",
    "val_dataset = CustomDataset(val_mfcc, val_labels, sequence_length=1)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcb73b-29c0-4088-8d6d-2d16a5c8b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8fd8a-8b67-4acf-9e5e-f85df9b0c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define deeper MLP model\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=CONFIG.N_CLASSES):\n",
    "        super(DeeperMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Define LSTM model with additional layers\n",
    "class EnhancedBiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=39, hidden_dim=128, output_dim=CONFIG.N_CLASSES):\n",
    "        super(EnhancedBiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True, num_layers=2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_lstm, _ = self.lstm(x)\n",
    "        output = self.fc(h_lstm[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Define Enhanced EnsembleModel\n",
    "class EnhancedEnsembleModel(nn.Module):\n",
    "    def __init__(self, cnn_model, lstm_model, output_dim=CONFIG.N_CLASSES):\n",
    "        super(EnhancedEnsembleModel, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = lstm_model\n",
    "        self.fc = nn.Linear(output_dim * 2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        cnn_output = self.cnn(x)\n",
    "        lstm_output = self.lstm(x.view(batch_size, 1, -1))\n",
    "        combined = torch.cat((cnn_output, lstm_output), dim=1)\n",
    "        output = self.fc(combined)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "# Training function with learning rate scheduling\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CONFIG.N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "\n",
    "        scheduler.step(_val_loss)\n",
    "\n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Validation function\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            probs = model(features)\n",
    "            loss = criterion(probs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "        _val_loss = np.mean(val_loss)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "\n",
    "    return _val_loss, auc_score\n",
    "\n",
    "# AUC calculation function\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "\n",
    "# Inference function\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "# Load test data and create DataLoader\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_mfcc = get_mfcc_feature(test, False)\n",
    "test_dataset = CustomDataset(test_mfcc, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model initialization and training\n",
    "input_dim = 39  # mfcc + chroma + spectral contrast + tonnetz\n",
    "cnn_model = DeeperMLP(input_dim=input_dim)\n",
    "lstm_model = EnhancedBiLSTM()\n",
    "ensemble_model = EnhancedEnsembleModel(cnn_model, lstm_model)\n",
    "optimizer = optim.AdamW(params=ensemble_model.parameters(), lr=CONFIG.LR, weight_decay=1e-5)\n",
    "infer_model = train(ensemble_model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Model inference\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "# Save submission\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38e4a8-2241-47f7-900f-dfe094df80e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject2",
   "language": "python",
   "name": "pythonproject2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
